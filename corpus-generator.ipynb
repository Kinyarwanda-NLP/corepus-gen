{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import nltk.data\n",
    "from progressbar import ProgressBar\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get vocabulary of size N from OPUS 50k frequency list\n",
    "\n",
    "def limit_opus_vocabulary(word_freq_file, size):\n",
    "    with open(word_freq_file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    words = [x.strip().split(' ')[0] for x in content]\n",
    "    word_freqs = [float(x.strip().split(' ')[1]) for x in content]\n",
    "    \n",
    "    vocab = []\n",
    "    freqs = []\n",
    "    for w, f in zip(words[0:size], word_freqs[0:size]):\n",
    "        vocab.append(w)\n",
    "        freqs.append(f)\n",
    "        \n",
    "    #normalize frequencies\n",
    "    sum_freqs = sum(freqs)\n",
    "    normalized_freqs = [f/sum_freqs for f in freqs]\n",
    "    \n",
    "    return vocab, normalized_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create vocabulary of top N frequent words\n",
    "REFERENCE_VOCAB_SIZE = 5000\n",
    "opus_reduced_vocab, opus_reduced_freqs = limit_opus_vocabulary('src/opus_en_50k.txt', REFERENCE_VOCAB_SIZE)\n",
    "opus_reduced_vocab_dict = {w:opus_reduced_freqs[i] for i, w in enumerate(opus_reduced_vocab)}\n",
    "\n",
    "#frequency plot on the opus vocabulary\n",
    "ax = sns.distplot(np.asarray(opus_reduced_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba filtering\n",
    "with open('etc/boynames.txt', 'r') as f:\n",
    "    boynames = f.readlines()\n",
    "    boynames = [name[0:-1] for name in boynames]\n",
    "with open('etc/girlnames.txt', 'r') as f:\n",
    "    girlnames = f.readlines()\n",
    "    girlnames = [name[0:-1] for name in girlnames]\n",
    "with open('etc/avoidlist.txt', 'r') as f:\n",
    "    avoidlist = f.readlines()\n",
    "    avoidlist = [word[0:-1] for word in avoidlist]\n",
    "    \n",
    "MAX_COMMAS = 9\n",
    "MAX_PERIOD = 6\n",
    "    \n",
    "def filter_tatoeba(sent_ids, sents, max_tokens, tom_replacement_set, mary_replacement_set, avoid_set = []):\n",
    "    filtered_ids = []\n",
    "    filtered_sents = []\n",
    "    filtered_sent_tokens = []\n",
    "    \n",
    "    pbar = ProgressBar()\n",
    "    for index in pbar(range(len(sent_ids))):\n",
    "        sent_id = sent_ids[index]\n",
    "        sent = sents[index]\n",
    "        sent_tokens = tokenize_en(sent, True, True)\n",
    "        if len(sent_tokens) <= max_tokens and sent_tokens.count(',') < MAX_COMMAS and sent_tokens.count('.') < MAX_PERIOD and len(intersection(avoidlist,sent_tokens)) == 0:\n",
    "            clean_sent = sent\n",
    "            #Name replacement - tatoeba corpus has an intense use of these two names\n",
    "            if 'tom' in sent_tokens:\n",
    "                clean_sent = clean_sent.replace('Tom', tom_replacement_set[random.randint(0,len(tom_replacement_set) - 1)])\n",
    "            if 'mary' in sent_tokens:\n",
    "                clean_sent = clean_sent.replace('Mary', mary_replacement_set[random.randint(0,len(mary_replacement_set) - 1)])\n",
    "                \n",
    "            filtered_sents.append(clean_sent)\n",
    "            filtered_ids.append(int(sent_id))\n",
    "            filtered_sent_tokens.append(sent_tokens)\n",
    "            \n",
    "    return filtered_sents, filtered_sent_tokens, filtered_ids\n",
    "\n",
    "def filter_selected_sents(sents, ids, skip_sent_ids):\n",
    "    filtered_sents = []\n",
    "    filtered_ids = []\n",
    "    for i, s in zip(ids, sents):\n",
    "        if i not in skip_sent_ids:\n",
    "            filtered_sents.append(clean_sent)\n",
    "            filtered_ids.append(sent_id)\n",
    "    return filtered_sents, filtered_ids\n",
    "    \n",
    "#Tokenization and vocab building tools\n",
    "tokenizer_en = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "def tokenize_en(string, to_lower = False, clear_nonalpha = False):\n",
    "    tokens = []\n",
    "    if to_lower:\n",
    "        string = string.lower()\n",
    "    string = string.strip()\n",
    "\n",
    "    for sent in tokenizer_en.tokenize(string):\n",
    "        if clear_nonalpha:\n",
    "            sent_tokens = [tok for tok in toktok.tokenize(sent) if tok.isalpha()]\n",
    "        else:\n",
    "            sent_tokens = [tok for tok in toktok.tokenize(sent)]\n",
    "        \n",
    "        tokens.extend(sent_tokens)\n",
    "    return tokens\n",
    "\n",
    "def get_vocabulary_from_corpus(sentence_tokens):\n",
    "    vocab_dict = {}\n",
    "    total_words = 0\n",
    "    for tokens in sentence_tokens:\n",
    "        word_tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "        for word_token in word_tokens:\n",
    "            if word_token in vocab_dict:\n",
    "                vocab_dict[word_token] += 1\n",
    "            else:\n",
    "                vocab_dict[word_token] = 1\n",
    "            total_words += 1\n",
    "                \n",
    "    sorted_vocab, sorted_freqs = sort_lists_wrt_list(list(vocab_dict.keys()), list(vocab_dict.values()))\n",
    "\n",
    "    #normalize frequencies \n",
    "    #sum_freqs = sum(sorted_freqs)\n",
    "    normalized_freqs = [float(f)/float(total_words) for f in sorted_freqs]\n",
    "    \n",
    "    print(\"Total #words:\", total_words)\n",
    "    \n",
    "    return sorted_vocab, normalized_freqs, total_words\n",
    "\n",
    "def sort_lists_wrt_list(mylist, values, ascending=False):\n",
    "    mylist_array = np.array(mylist)\n",
    "    values_array = np.array(values)  \n",
    "    \n",
    "    if ascending:\n",
    "        inds = values_array.argsort()[::]\n",
    "    else:\n",
    "        inds = values_array.argsort()[::-1]\n",
    "    \n",
    "    mylist_sorted = mylist_array[inds]\n",
    "    values_sorted = values_array[inds]\n",
    "    \n",
    "    return list(mylist_sorted), list(values_sorted)\n",
    "\n",
    "def sort_multilists_wrt_list(mylists, values, ascending=False):\n",
    "    mylists_arrays = [np.array(mylist) for mylist in mylists]\n",
    "    values_array = np.array(values)  \n",
    "    \n",
    "    if ascending:\n",
    "        inds = values_array.argsort()[::]\n",
    "    else:\n",
    "        inds = values_array.argsort()[::-1]\n",
    "    \n",
    "    mylists_sorted = [mylist_array[inds] for mylist_array in mylists_arrays]\n",
    "    values_sorted = values_array[inds]\n",
    "    \n",
    "    return [list(mylist_sorted) for mylist_sorted in mylists_sorted], list(values_sorted)\n",
    "\n",
    "def shuffle_parallel_lists(list_of_lists):\n",
    "    list_of_arrays = [np.array(l) for l in list_of_lists]\n",
    "    inds = list(range(0, len(list_of_arrays[0])))\n",
    "    random.shuffle(inds)\n",
    "    return [list(l[inds]) for l in list_of_arrays]\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return list(set(lst3))\n",
    "\n",
    "def difference(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value not in lst2] \n",
    "    return list(set(lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tatoeba 1196198\n"
     ]
    }
   ],
   "source": [
    "#Read source set\n",
    "tatoeba_corpus_path = \"src/tatoeba_sentences_EN.csv\"\n",
    "with open(tatoeba_corpus_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "all_tatoeba_sent_info = [[x.strip().split(\"\\t\")[0], x.strip().split(\"\\t\")[2]] for x in content]\n",
    "all_tatoeba_sent_ids = [i[0]for i in all_tatoeba_sent_info]\n",
    "all_tatoeba_sents = [i[1]for i in all_tatoeba_sent_info]\n",
    "print('all tatoeba', len(all_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean tatoeba 1190776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Clean the source set\n",
    "MAX_TOKENS = 40\n",
    "clean_tatoeba_sents, clean_tatoeba_sent_tokens, clean_tatoeba_ids = filter_tatoeba(all_tatoeba_sent_ids, \n",
    "                                                                                   all_tatoeba_sents, \n",
    "                                                                                   MAX_TOKENS, \n",
    "                                                                                   boynames, \n",
    "                                                                                   girlnames, \n",
    "                                                                                   avoidlist)\n",
    "\n",
    "print('clean tatoeba', len(clean_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read exclude sets\n",
    "exclude_set_files = ['etc/exclude.txt']\n",
    "exclude_set = []\n",
    "for file in exclude_set_files:\n",
    "    to_exclude = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        to_exclude = [int(idx[0:-1]) for idx in lines]\n",
    "    exclude_set.extend(to_exclude)\n",
    "\n",
    "#Sort it\n",
    "exclude_set, _ = sort_lists_wrt_list(exclude_set, exclude_set, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all set size 1190776\n",
      "exclude set size 5000\n",
      "filtered tatoeba 1185776\n"
     ]
    }
   ],
   "source": [
    "#Filter previously stored sentences (exclude_set)\n",
    "print('all set size', len(clean_tatoeba_ids))\n",
    "print('exclude set size', len(exclude_set))\n",
    "\n",
    "filtered_tatoeba_sents = []\n",
    "filtered_tatoeba_sent_tokens = []\n",
    "filtered_tatoeba_ids = []\n",
    "\n",
    "all_pointer = 0\n",
    "exclude_pointer = 0\n",
    "\n",
    "while all_pointer < len(clean_tatoeba_ids):\n",
    "    all_id = clean_tatoeba_ids[all_pointer]\n",
    "    if exclude_pointer == len(exclude_set): #already at the end of exclude set take the rest from all set\n",
    "        filtered_tatoeba_sents.append(clean_tatoeba_sents[all_pointer])\n",
    "        filtered_tatoeba_sent_tokens.append(clean_tatoeba_sent_tokens[all_pointer])\n",
    "        filtered_tatoeba_ids.append(clean_tatoeba_ids[all_pointer])\n",
    "        \n",
    "        all_pointer += 1\n",
    "    else:\n",
    "        exclude_id = exclude_set[exclude_pointer]\n",
    "        while all_id < exclude_id:\n",
    "            filtered_tatoeba_sents.append(clean_tatoeba_sents[all_pointer])\n",
    "            filtered_tatoeba_sent_tokens.append(clean_tatoeba_sent_tokens[all_pointer])\n",
    "            filtered_tatoeba_ids.append(clean_tatoeba_ids[all_pointer])\n",
    "\n",
    "            all_pointer += 1\n",
    "            all_id = clean_tatoeba_ids[all_pointer]\n",
    "\n",
    "        all_pointer += 1\n",
    "        exclude_pointer += 1\n",
    "\n",
    "print('filtered tatoeba', len(filtered_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive corpus selection: Select C random sentences\n",
    "CORPUS_SIZE = 10000\n",
    "shuffled_sents, shuffled_sent_tokens, shuffled_ids = shuffle_parallel_lists([filtered_tatoeba_sents, filtered_tatoeba_sent_tokens, filtered_tatoeba_ids])\n",
    "\n",
    "naive_corpus_sents = shuffled_sents[0:CORPUS_SIZE]\n",
    "naive_corpus_sent_tokens = shuffled_sent_tokens[0:CORPUS_SIZE]\n",
    "naive_corpus_ids = shuffled_ids[0:CORPUS_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #words: 79181\n",
      "corpus size 10000\n",
      "corpus_vocab 6169\n",
      "reference vocab 5000\n",
      "intersecting 3297\n",
      "missing 1703\n",
      "extra 2872\n",
      "34.060000\n"
     ]
    }
   ],
   "source": [
    "#Build vocabulary on the corpus\n",
    "corpus = naive_corpus_sents\n",
    "corpus_tokenized = naive_corpus_sent_tokens\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "corpus_vocab, corpus_freqs, corpus_total_words = get_vocabulary_from_corpus(corpus_tokenized)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, corpus_vocab)\n",
    "missing_vocab = difference(reference_vocab, corpus_vocab)\n",
    "extra_vocab = difference(corpus_vocab, reference_vocab)\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"missing\", len(missing_vocab))\n",
    "print(\"extra\", len(extra_vocab))\n",
    "print(\"%f\"%(len(missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USED\n",
    "#Word frequency difference analysis\n",
    "freq_differences_word = []\n",
    "freq_differences_diff = []\n",
    "for w_corpus, f_corpus in zip(corpus_vocab, corpus_freqs):\n",
    "    try:\n",
    "        f_reference = opus_freqs[opus_vocab.index(w_corpus)]\n",
    "        #print(\"%s - f_corp=%f, f_ref=%f\"%(w_corpus, f_corpus, f_reference))\n",
    "        freq_difference = f_reference - f_corpus\n",
    "        freq_differences_word.append(w_corpus)\n",
    "        freq_differences_diff.append(freq_difference)\n",
    "    except:\n",
    "        f_reference = 0\n",
    "\n",
    "freq_differences_word_array = np.array(freq_differences_word)\n",
    "freq_differences_diff_array = np.array(freq_differences_diff)\n",
    "inds = freq_differences_diff_array.argsort()[::-1]\n",
    "    \n",
    "sorted_freq_differences_word = freq_differences_word_array[inds]\n",
    "sorted_freq_differences_diff = freq_differences_diff_array[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USED\n",
    "#Word frequency difference analysis - continued\n",
    "threshold = 0.001\n",
    "for i, (w, d) in enumerate(zip(sorted_freq_differences_word, sorted_freq_differences_diff)):\n",
    "    opus_freq = opus_freqs[opus_vocab.index(w)]\n",
    "    need = int(float(corpus_total_words) * d)\n",
    "    if d >= threshold or d <= (-1* threshold):\n",
    "        print(\"%d: %s - diff:%f, need:%d\"%(i, w, d, need))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out missing words in the order that they appear in opus vocabulary\n",
    "missing_indexes = [opus_vocab.index(w) for w in still_missing_vocab]\n",
    "sorted_missing_vocab, sorted_missing_index = sort_lists_wrt_list(missing_vocab, missing_indexes, True)\n",
    "\n",
    "for w, index in zip(sorted_missing_vocab, sorted_missing_index):\n",
    "    print(\"%s - opus ind:%i\"%(w, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #words: 43559\n",
      "corpus size 5633\n",
      "corpus_vocab 6169\n",
      "reference vocab 5000\n",
      "intersecting 2478\n",
      "missing 2522\n",
      "extra 0\n",
      "50.440000\n"
     ]
    }
   ],
   "source": [
    "#Take out sentences in the corpus: those with non-frequent words and randomly\n",
    "RANDOM_SENTENCE_TAKEOUT = 4 #size of die to roll, put -1 to skip\n",
    "count = 0\n",
    "naive_corpus_sents_nonfrequent_takenout = []\n",
    "naive_corpus_ids_nonfrequent_takenout = []\n",
    "naive_corpus_senttokens_nonfrequent_takenout = []\n",
    "for sent, sent_tokens, sent_id in zip(naive_corpus_sents, naive_corpus_sent_tokens, naive_corpus_ids):\n",
    "    nonfrequent = False\n",
    "    for token in sent_tokens:\n",
    "        if token not in opus_reduced_vocab: #condition for non-frequent = not in opus_reduced_vocab\n",
    "            nonfrequent = True\n",
    "            break\n",
    "            \n",
    "    #Random sentence extraction\n",
    "    if not RANDOM_SENTENCE_TAKEOUT == -1:\n",
    "        dice = random.randint(0,RANDOM_SENTENCE_TAKEOUT)\n",
    "    else:\n",
    "        dice = 1\n",
    "\n",
    "    if not dice == 0:        \n",
    "        if nonfrequent:\n",
    "            count += 1\n",
    "        else:\n",
    "            naive_corpus_sents_nonfrequent_takenout.append(sent)\n",
    "            naive_corpus_ids_nonfrequent_takenout.append(sent_id)\n",
    "            naive_corpus_senttokens_nonfrequent_takenout.append(sent_tokens)\n",
    "\n",
    "#Build vocabulary on the corpus\n",
    "corpus = naive_corpus_sents_nonfrequent_takenout\n",
    "corpus_tokenized = naive_corpus_senttokens_nonfrequent_takenout\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "reduced_corpus_vocab, reduced_corpus_freqs, reduced_corpus_total_words = get_vocabulary_from_corpus(corpus_tokenized)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, reduced_corpus_vocab)\n",
    "missing_vocab = difference(reference_vocab, reduced_corpus_vocab)\n",
    "extra_vocab = difference(reduced_corpus_vocab, reference_vocab)\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"missing\", len(missing_vocab))\n",
    "print(\"extra\", len(extra_vocab))\n",
    "print(\"%f\"%(len(missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no candidate fill-in sents 110313\n"
     ]
    }
   ],
   "source": [
    "#Search in rest of the sentences in tatoeba which has the missing words\n",
    "missing_vocab_replacement_coefs = [(REFERENCE_VOCAB_SIZE - opus_reduced_vocab.index(token))/1000 if token in missing_vocab else 0.0 for token in opus_reduced_vocab]\n",
    "original_missing_vocab_replacement_coefs = missing_vocab_replacement_coefs.copy()\n",
    "\n",
    "count = 0 \n",
    "no_missing_in_candidate = []\n",
    "candidate_replacement_score = []\n",
    "candidate_sent_index = []\n",
    "candidate_missing_word_indexes = []\n",
    "candidate_missing_word_coefs = []\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(filtered_tatoeba_sents))):\n",
    "    if i < CORPUS_SIZE:\n",
    "        continue\n",
    "        \n",
    "    sent = filtered_tatoeba_sents[i]    \n",
    "    sent_tokens = filtered_tatoeba_sent_tokens[i]\n",
    "    missing_in_sent = intersection(sent_tokens, missing_vocab)\n",
    "    no_missing = len(missing_in_sent)\n",
    "    extra_in_sent = difference(sent_tokens, opus_reduced_vocab)\n",
    "    no_extra = len(extra_in_sent)\n",
    "\n",
    "    \n",
    "    missing_word_indexes = [opus_reduced_vocab.index(token) for token in missing_in_sent]\n",
    "    missing_word_replacement_coefs = [missing_vocab_replacement_coefs[token_index] for token_index in missing_word_indexes]    \n",
    "    replacement_score = sum(missing_word_replacement_coefs)\n",
    "    \n",
    "    \n",
    "    if no_missing > 0 and no_extra == 0 :\n",
    "        no_missing_in_candidate.append(no_missing)\n",
    "        candidate_replacement_score.append(replacement_score)\n",
    "        candidate_sent_index.append(i)\n",
    "        candidate_missing_word_indexes.append(missing_word_indexes)\n",
    "        candidate_missing_word_coefs.append(missing_word_replacement_coefs)\n",
    "        count += 1\n",
    "    \n",
    "[candidate_sent_index_sorted, candidate_missing_word_coefs_sorted, candidate_missing_word_indexes_sorted], candidate_replacement_score_sorted = sort_multilists_wrt_list([candidate_sent_index, candidate_missing_word_coefs, candidate_missing_word_indexes], candidate_replacement_score)    \n",
    "        \n",
    "print(\"no candidate fill-in sents\", len(candidate_sent_index_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To restore original coefficients before rescoring\n",
    "missing_vocab_replacement_coefs = original_missing_vocab_replacement_coefs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescore candidates. Replacement coefficient of a missing word is reduced by once it is seen. \n",
    "new_candidate_replacement_score = []\n",
    "new_candidate_missing_word_coefs = []\n",
    "for candidate_score, candidate_sent, covered_indexes in zip(candidate_replacement_score_sorted, candidate_sent_index_sorted, candidate_missing_word_indexes_sorted):\n",
    "\n",
    "    new_missing_word_replacement_coefs = [missing_vocab_replacement_coefs[token_index] for token_index in covered_indexes]\n",
    "    new_replacement_score = sum(new_missing_word_replacement_coefs)\n",
    "    new_candidate_missing_word_coefs.append(new_missing_word_replacement_coefs)\n",
    "    new_candidate_replacement_score.append(new_replacement_score)\n",
    "    \n",
    "    #Reduce coef of less frequent words more than frequent words in order to maintain frequency\n",
    "    for token_index in covered_indexes:\n",
    "        if token_index < 1000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*50/100\n",
    "        elif token_index < 2000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*60/100\n",
    "        elif token_index < 2000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*70/100\n",
    "        elif token_index < 3000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*80/100\n",
    "        elif token_index < 4000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*90/100\n",
    "        elif token_index < 5000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "        elif token_index < 6000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "        elif token_index < 7000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "\n",
    "\n",
    "[new_candidate_sent_index_sorted, new_candidate_missing_word_coefs_sorted, new_candidate_missing_word_indexes_sorted], new_candidate_replacement_score_sorted = sort_multilists_wrt_list([candidate_sent_index_sorted, candidate_missing_word_coefs_sorted, candidate_missing_word_indexes_sorted], new_candidate_replacement_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print replacement candidate sentences sorted with their score sum\n",
    "\n",
    "# sentence_index = candidate_sent_index_sorted\n",
    "# sentence_coefs = candidate_missing_word_coefs_sorted\n",
    "# sentence_missing_words = candidate_missing_word_indexes_sorted\n",
    "# sentence_score = candidate_replacement_score_sorted\n",
    "\n",
    "sentence_index = new_candidate_sent_index_sorted\n",
    "sentence_coefs = new_candidate_missing_word_coefs_sorted\n",
    "sentence_missing_words = new_candidate_missing_word_indexes_sorted\n",
    "sentence_score = new_candidate_replacement_score_sorted\n",
    "\n",
    "count = 0\n",
    "no_fill_in = CORPUS_SIZE - len(naive_corpus_sents_nonfrequent_takenout)\n",
    "for s_index, s_score, missing_words, coefs in zip(sentence_index, sentence_score, sentence_missing_words, sentence_coefs):\n",
    "    missing_info=\"\"\n",
    "    for token_index, coef in zip(missing_words, coefs):\n",
    "        missing_info += opus_reduced_vocab[token_index]\n",
    "        missing_info += \"(\"\n",
    "        missing_info += str(token_index)\n",
    "        missing_info += \"/\"\n",
    "        missing_info += str(coef)\n",
    "        missing_info += \") \"\n",
    "    \n",
    "    if no_fill_in - count <= 0:\n",
    "        print(\"=========================================================\")\n",
    "    \n",
    "    print(\"%s -- %f %s\"%(filtered_tatoeba_sents[s_index], s_score, missing_info))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check duplication in fill-in set\n",
    "fillin_from = new_candidate_sent_index_sorted\n",
    "fillin_from_tokens = [filtered_tatoeba_sent_tokens[index] for index in new_candidate_sent_index_sorted]\n",
    "\n",
    "cover_dict = {missing_word:0 for missing_word in missing_vocab}\n",
    "for sent_tokens in fillin_from_tokens:\n",
    "    for tok in sent_tokens:\n",
    "        if tok in cover_dict.keys():\n",
    "            cover_dict[tok] += 1\n",
    "            if cover_dict[tok] > 1:\n",
    "                print(\"%s is covered %i times\"%(tok, cover_dict[tok]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced corpus 5633\n",
      "filled-in corpus 10000\n"
     ]
    }
   ],
   "source": [
    "#Fill in candidate sentences to the reduced corpus\n",
    "\n",
    "fillin_from = new_candidate_sent_index_sorted\n",
    "fillin_from_tokens = [filtered_tatoeba_sent_tokens[index] for index in new_candidate_sent_index_sorted]\n",
    "\n",
    "naive_corpus_sents_frequent_filledin = naive_corpus_sents_nonfrequent_takenout.copy()\n",
    "naive_corpus_senttokens_frequent_filledin = naive_corpus_senttokens_nonfrequent_takenout.copy()\n",
    "naive_corpus_ids_frequent_filledin = naive_corpus_ids_nonfrequent_takenout.copy()\n",
    "print(\"reduced corpus\", len(naive_corpus_sents_nonfrequent_takenout))\n",
    "for sent_index in new_candidate_sent_index_sorted:\n",
    "    sent = filtered_tatoeba_sents[sent_index]\n",
    "    sent_id = filtered_tatoeba_ids[sent_index]\n",
    "    sent_tokens = filtered_tatoeba_sent_tokens[sent_index]\n",
    "    naive_corpus_sents_frequent_filledin.append(sent)\n",
    "    naive_corpus_senttokens_frequent_filledin.append(sent_tokens)\n",
    "    naive_corpus_ids_frequent_filledin.append(sent_id)\n",
    "    if len(naive_corpus_sents_frequent_filledin) >= CORPUS_SIZE:\n",
    "        break\n",
    "print(\"filled-in corpus\", len(naive_corpus_sents_frequent_filledin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #words: 85140\n",
      "corpus size 10000\n",
      "corpus_vocab 4809\n",
      "reference vocab 5000\n",
      "intersecting 4809\n",
      "still missing 191\n",
      "still extra 0\n",
      "3.820000\n"
     ]
    }
   ],
   "source": [
    "#Filled-in corpus stats\n",
    "corpus = naive_corpus_sents_frequent_filledin\n",
    "corpus_tokens = naive_corpus_senttokens_frequent_filledin\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "modified_corpus_vocab, modified_corpus_freqs, modified_corpus_total_words = get_vocabulary_from_corpus(corpus_tokens)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, modified_corpus_vocab)\n",
    "still_missing_vocab = difference(reference_vocab, modified_corpus_vocab)\n",
    "still_extra_vocab = difference(modified_corpus_vocab, reference_vocab)\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(modified_corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"still missing\", len(still_missing_vocab))\n",
    "print(\"still extra\", len(still_extra_vocab))\n",
    "print(\"%f\"%(len(still_missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write corpus to file (run after doing counts)\n",
    "\n",
    "CORPUS_ID = 'testcore/testcore'\n",
    "corpus_sents_to_write = naive_corpus_sents_frequent_filledin\n",
    "corpus_ids_to_write = naive_corpus_ids_frequent_filledin\n",
    "\n",
    "with open(CORPUS_ID +'_sentences.txt', 'w') as f:\n",
    "    for s in corpus_sents_to_write:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(CORPUS_ID +'_ids.txt', 'w') as f:\n",
    "    for s in corpus_ids_to_write:\n",
    "        #print(type(s))\n",
    "        f.write(str(s) + \"\\n\")\n",
    "    \n",
    "with open(CORPUS_ID +'_vocab.txt', 'w') as f:\n",
    "    for s in corpus_vocab:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(CORPUS_ID +'_missingvocab.txt', 'w') as f:\n",
    "    for s in missing_vocab:\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
